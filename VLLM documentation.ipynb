{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c25e71-25c3-438d-bb6e-7cd32236757f",
   "metadata": {},
   "source": [
    "## class LLM\n",
    "\n",
    "An LLM for generating texts from given prompts and sampling parameters.\n",
    "\n",
    "This class includes a tokenizer, a language model (possibly distributed across multiple GPUs), and GPU memory space allocated for intermediate states (aka KV cache). Given a batch of prompts and sampling parameters, this class generates texts from the model, using an intelligent batching mechanism and efficient memory management.\n",
    "\n",
    "NOTE: This class is intended to be used for offline inference. For online serving, use the `AsyncLLMEngine` class instead.\n",
    "\n",
    "NOTE: For the comprehensive list of arguments, see `EngineArgs`.\n",
    "\n",
    "### __init__()\n",
    "\n",
    "- model: The name or path of a HuggingFace Transformers model.\n",
    "- tokenizer: The name or path of a HuggingFace Transformers tokenizer.\n",
    "- tokenizer_mode: The tokenizer mode. \"auto\" will use the fast tokenizer if available, and \"slow\" will always use the slow tokenizer.\n",
    "- skip_tokenizer_init: If true, skip initialization of tokenizer and detokenizer. Expect valid prompt_token_ids and None for prompt from the input.\n",
    "- trust_remote_code: Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer.\n",
    "- tensor_parallel_size: The number of GPUs to use for distributed execution with tensor parallelism.\n",
    "- dtype: The data type for the model weights and activations. Currently, we support `float32`, `float16`, and `bfloat16`. If `auto`, we use the `torch_dtype` attribute specified in the model config file. However, if the `torch_dtype` in the config is `float32`, we will use `float16` instead.\n",
    "- quantization: The method used to quantize the model weights. Currently, we support \"awq\", \"gptq\", \"squeezellm\", and \"fp8\" (experimental). If None, we first check the `quantization_config` attribute in the model config file. If that is None, we assume the model weights are not quantized and use `dtype` to determine the data type of the weights.\n",
    "- revision: The specific model version to use. It can be a branch name, a tag name, or a commit id.\n",
    "- tokenizer_revision: The specific tokenizer version to use. It can be a branch name, a tag name, or a commit id.\n",
    "- seed: The seed to initialize the random number generator for sampling.\n",
    "- gpu_memory_utilization: The ratio (between 0 and 1) of GPU memory to reserve for the model weights, activations, and KV cache. Higher values will increase the KV cache size and thus improve the model's throughput. However, if the value is too high, it may cause out-of- memory (OOM) errors.\n",
    "- swap_space: The size (GiB) of CPU memory per GPU to use as swap space. This can be used for temporarily storing the states of the requests when their `best_of` sampling parameters are larger than 1. If all requests will have `best_of=1`, you can safely set this to 0. Otherwise, too small values may cause out-of-memory (OOM) errors.\n",
    "- enforce_eager: Whether to enforce eager execution. If True, we will disable CUDA graph and always execute the model in eager mode. If False, we will use CUDA graph and eager execution in hybrid.\n",
    "- max_context_len_to_capture: Maximum context len covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode (DEPRECATED. Use `max_seq_len_to_capture` instead).\n",
    "- max_seq_len_to_capture: Maximum sequence len covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode.\n",
    "- disable_custom_all_reduce: See ParallelConfig\n",
    "\n",
    "Types\n",
    "\n",
    "- model: str,\n",
    "- tokenizer: Optional[str] = None,\n",
    "- tokenizer_mode: str = \"auto\",\n",
    "- skip_tokenizer_init: bool = False,\n",
    "- trust_remote_code: bool = False,\n",
    "- tensor_parallel_size: int = 1,\n",
    "- dtype: str = \"auto\",\n",
    "- quantization: Optional[str] = None,\n",
    "- revision: Optional[str] = None,\n",
    "- tokenizer_revision: Optional[str] = None,\n",
    "- seed: int = 0,\n",
    "- gpu_memory_utilization: float = 0.9,\n",
    "- swap_space: int = 4,\n",
    "- enforce_eager: bool = False,\n",
    "- max_context_len_to_capture: Optional[int] = None,\n",
    "- max_seq_len_to_capture: int = 8192,\n",
    "- disable_custom_all_reduce: bool = False,\n",
    "- **kwargs,\n",
    "\n",
    "Creates an LLM engine from the engine arguments.\n",
    "\n",
    "- engine_args = EngineArgs(...)\n",
    "- self.llm_engine = LLMEngine.from_engine_args(engine_args, usage_context=UsageContext.LLM_CLASS)\n",
    "- self.request_counter = Counter()\n",
    "\n",
    "### generate()\n",
    "\n",
    "- prompts: A list of prompts to generate completions for.\n",
    "- sampling_params: The sampling parameters for text generation. If None, we use the default sampling parameters. When it is a single value, it is applied to every prompt. When it is a list, the list must have the same length as the prompts and it is paired one by one with the prompt.\n",
    "- prompt_token_ids: A list of token IDs for the prompts. If None, we use the tokenizer to convert the prompts to token IDs.\n",
    "- use_tqdm: Whether to use tqdm to display the progress bar.\n",
    "- lora_request: LoRA request to use for generation, if any.\n",
    "- multi_modal_data: Multi modal data.\n",
    "\n",
    "Types\n",
    "\n",
    "- prompts: Optional[Union[str, List[str]]] = None,\n",
    "- sampling_params: Optional[Union[SamplingParams, List[SamplingParams]]] = None,\n",
    "- prompt_token_ids: Optional[List[List[int]]] = None,\n",
    "- use_tqdm: bool = True,\n",
    "- lora_request: Optional[LoRARequest] = None,\n",
    "- multi_modal_data: Optional[MultiModalData] = None,\n",
    "\n",
    "Returns\n",
    "            \n",
    "A list of `RequestOutput` objects containing the generated completions in the same order as the input prompts.\n",
    "\n",
    "1. requests_data = self._validate_and_prepare_requests(...)\n",
    "2. for request_data in requests_data: self._add_request(**request_data)\n",
    "3. return self._run_engine(use_tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5bd6c-b58c-472d-8390-e7991631d316",
   "metadata": {},
   "source": [
    "## vllm/entrypoints/openai/api_server.py\n",
    "\n",
    "vLLM OpenAI-Compatible RESTful API server.\n",
    "\n",
    "Server arguments\n",
    "\n",
    "- argument(\"--host\", type=nullable_str, default=None, help=\"host name\")\n",
    "- argument(\"--port\", type=int, default=8000, help=\"port number\")\n",
    "- argument(\"--uvicorn-log-level\", type=str, default=\"info\", choices=['debug', 'info', 'warning', 'error', 'critical', 'trace'], help=\"log level for uvicorn\")\n",
    "- argument(\"--allow-credentials\", action=\"store_true\", help=\"allow credentials\")\n",
    "- argument(\"--allowed-origins\", type=json.loads, default=[\"*\"], help=\"allowed origins\")\n",
    "- argument(\"--allowed-methods\", type=json.loads, default=[\"*\"], help=\"allowed methods\")\n",
    "- argument(\"--allowed-headers\", type=json.loads, default=[\"*\"], help=\"allowed headers\")\n",
    "- argument(\"--api-key\", type=nullable_str, default=None, help=\"If provided, the server will require this key to be presented in the header.\")\n",
    "- argument(\"--lora-modules\", type=nullable_str, default=None, nargs='+', action=LoRAParserAction, help=\"LoRA module configurations in the format name=path. Multiple modules can be specified.\")\n",
    "- argument(\"--chat-template\", type=nullable_str, default=None, help=\"The file path to the chat template, or the template in single-line form for the specified model\")\n",
    "- argument(\"--response-role\", type=nullable_str, default=\"assistant\", help=\"The role name to return if `request.add_generation_prompt=true`.\")\n",
    "- argument(\"--ssl-keyfile\", type=nullable_str, default=None, help=\"The file path to the SSL key file\")\n",
    "- argument(\"--ssl-certfile\", type=nullable_str, default=None, help=\"The file path to the SSL cert file\")\n",
    "- argument(\"--ssl-ca-certs\", type=nullable_str, default=None, help=\"The CA certificates file\")\n",
    "- argument( \"--ssl-cert-reqs\", type=int, efault=int(ssl.CERT_NONE), help=\"Whether client certificate is required (see stdlib ssl module's)\")\n",
    "- argument(\"--root-path\", type=nullable_str, default=None, help=\"FastAPI root_path when app is behind a path based routing proxy\")\n",
    "- argument( \"--middleware\", type=nullable_str, action=\"append\", default=[], help=\"Additional ASGI middleware to apply to the app. We accept multiple --middleware arguments. The value should be an import path. If a function is provided, vLLM will add it to the server \"using @app.middleware('http'). If a class is provided, vLLM will add it to the server using app.add_middleware().\")\n",
    "\n",
    "AsyncEngine arguments\n",
    "\n",
    "- argument('--engine-use-ray', action='store_true', help='Use Ray to start the LLM engine in a separate process as the server process.')\n",
    "- argument('--disable-log-requests', action='store_true', help='Disable logging requests.')\n",
    "- argument('--max-log-len', type=int, default=None, help='Max number of prompt characters or prompt  ID numbers being printed in log. Default: Unlimited')\n",
    "\n",
    "Engine arguments\n",
    "\n",
    "> Model arguments\n",
    "\n",
    "- argument('--model', type=str, default='facebook/opt-125m', help='Name or path of the huggingface model to use.')\n",
    "- argument('--tokenizer', type=nullable_str, default=EngineArgs.tokenizer, help='Name or path of the huggingface tokenizer to use.')\n",
    "- argument('--skip-tokenizer-init', action='store_true', help='Skip initialization of tokenizer and detokenizer')\n",
    "- argument('--revision', type=nullable_str, default=None, help='The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.')\n",
    "- argument('--code-revision', type=nullable_str, default=None, help='The specific revision to use for the model code on Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.')\n",
    "- argument('--tokenizer-revision', type=nullable_str, default=None, help='The specific tokenizer version to use. It can be a branch  name, a tag name, or a commit id. If unspecified, will use the default version.')\n",
    "- argument('--tokenizer-mode', type=str, default=EngineArgs.tokenizer_mode, choices=['auto', 'slow'], help='The tokenizer mode.\\n\\n* \"auto\" will use the fast tokenizer if available.\\n* \"slow\" will always use the slow tokenizer.')\n",
    "- argument('--trust-remote-code', action='store_true', help='Trust remote code from huggingface.')\n",
    "- argument('--download-dir', type=nullable_str, default=EngineArgs.download_dir, help='Directory to download and load the weights, default to the default cache dir of huggingface.')\n",
    "- argument('--load-format', type=str, default=EngineArgs.load_format, choices=['auto', 'pt', 'safetensors', 'npcache', 'dummy', 'tensorizer'], help='The format of the model weights to load.\\n\\n * \"auto\" will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available.\\n * \"pt\" will load the weights in the pytorch bin format.\\n * \"safetensors\" will load the weights in the safetensors format.\\n * \"npcache\" will load the weights in pytorch format and store a numpy cache to speed up the loading.\\n * \"dummy\" will initialize the weights with random values, which is mainly for profiling.\\n * \"tensorizer\" will load the weights using tensorizer from CoreWeave. See the Tensorize vLLM Model script in the Examplessection for more information.\\n')\n",
    "- argument('--dtype', type=str, default=EngineArgs.dtype, choices=['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'], help='Data type for model weights and activations.\\n\\n * \"auto\" will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models.\\n * \"half\" for FP16. Recommended for AWQ quantization.\\n * \"float16\" is the same as \"half\".\\n * \"bfloat16\" for a balance between precision and range.\\n * \"float\" is shorthand for FP32 precision.\\n * \"float32\" for FP32 precision.')\n",
    "- argument('--max-model-len', type=int, default=EngineArgs.max_model_len, help='Model context length. If unspecified, will be automatically derived from the model config.')\n",
    "- argument('--rope-scaling', default=None, type=json.loads, help='RoPE scaling configuration in JSON format. For example, {\"type\":\"dynamic\",\"factor\":2.0}')\n",
    "- argument('--model-loader-extra-config', type=nullable_str, default=EngineArgs.model_loader_extra_config, help='Extra config for model loader. This will be passed to the model loader corresponding to the chosen load_format. This should be a JSON string that will be parsed into a dictionary.')\n",
    "- argument(\"--served-model-name\", nargs=\"+\", type=str, default=None, help=\"The model name(s) used in the API. If multiple names are provided, the server will respond to any of the provided names. The model name in the model field of a response will be the first name in this list. If not specified, the model name will be the same as the `--model` argument. Noted that this name(s) will also be used in `model_name` tag content of prometheus metrics, if multiple names provided, metricstag will take the first one.\")\n",
    "\n",
    "> Guided decoding\n",
    "\n",
    "- argument('--guided-decoding-backend', type=str, default='outlines', choices=['outlines', 'lm-format-enforcer'], help='Which engine will be used for guided decoding (JSON schema / regex etc) by default. Currently support https://github.com/outlines-dev/outlines and https://github.com/noamgat/lm-format-enforcer. Can be overridden per request via guided_decoding_backend parameter.')\n",
    "\n",
    "> Parallel arguments\n",
    "\n",
    "- argument('--distributed-executor-backend', choices=['ray', 'mp'], default=EngineArgs.distributed_executor_backend, help='Backend to use for distributed serving. When more than 1 GPU is used, will be automatically set to \"ray\" if installed or \"mp\" (multiprocessing) otherwise.')\n",
    "- argument('--worker-use-ray', action='store_true', help='Deprecated, use --distributed-executor-backend=ray.')\n",
    "- argument('--pipeline-parallel-size', '-pp', type=int, default=EngineArgs.pipeline_parallel_size, help='Number of pipeline stages.')\n",
    "- argument('--tensor-parallel-size', '-tp', type=int, default=EngineArgs.tensor_parallel_size, help='Number of tensor parallel replicas.')\n",
    "- argument('--max-parallel-loading-workers', type=int, default=EngineArgs.max_parallel_loading_workers, help='Load model sequentially in multiple batches, to avoid RAM OOM when using tensor parallel and large models.')\n",
    "- argument('--ray-workers-use-nsight', action='store_true', help='If specified, use nsight to profile Ray workers.')\n",
    "- argument('--disable-custom-all-reduce', action='store_true', default=EngineArgs.disable_custom_all_reduce, help='See ParallelConfig.')\n",
    "\n",
    "> CPU / GPU optimization\n",
    "\n",
    "- argument(\"--device\", type=str, default=EngineArgs.device, choices=[\"auto\", \"cuda\", \"neuron\", \"cpu\"], help='Device type for vLLM execution.')\n",
    "- argument('--gpu-memory-utilization', type=float, default=EngineArgs.gpu_memory_utilization, help='The fraction of GPU memory to be used for the model executor, which can range from 0 to 1. For example, a value of 0.5 would imply 50%% GPU memory utilization. If unspecified, will use the default value of 0.9.')\n",
    "- argument('--num-gpu-blocks-override', type=int, default=None, help='If specified, ignore GPU profiling result and use this number of GPU blocks. Used for testing preemption.')\n",
    "- argument('--enforce-eager', action='store_true', help='Always use eager-mode PyTorch. If False, will use eager mode and CUDA graph in hybrid for maximal performance and flexibility.')\n",
    "- argument('--max-seq-len-to-capture', type=int, default=EngineArgs.max_seq_len_to_capture, help='Maximum sequence length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode.')\n",
    "- argument('--swap-space', type=int, default=EngineArgs.swap_space, help='CPU swap space size (GiB) per GPU.')\n",
    "- argument('--seed', type=int, default=EngineArgs.seed, help='Random seed for operations.')\n",
    "\n",
    "> Quantization settings.\n",
    "\n",
    "- argument('--quantization', '-q', type=nullable_str, choices=[*QUANTIZATION_METHODS, None], default=EngineArgs.quantization, help='Method used to quantize the weights. If None, we first check the `quantization_config` attribute in the model config file. If that is None, we assume the model weights are not quantized and use `dtype` to determine the data type of the weights.')\n",
    "- argument('--kv-cache-dtype', type=str, choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3'], default=EngineArgs.kv_cache_dtype, help='Data type for kv cache storage. If \"auto\", will use model  data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. ROCm (AMD GPU) supports fp8 (=fp8_e4m3)')\n",
    "- argument('--quantization-param-path' type=nullable_str, default=None, help='Path to the JSON file containing the KV cache scaling factors. This should generally be supplied, when 'KV cache dtype is FP8. Otherwise, KV cache scaling factors default to 1.0, which may cause accuracy issues. FP8_E5M2 (without scaling) is only supported on cuda version greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead supported for common inference criteria.')\n",
    "\n",
    "> Speculative decoding\n",
    "\n",
    "- argument('--speculative-model', type=nullable_str, default=EngineArgs.speculative_model, help='The name of the draft model to be used in speculative decoding.')\n",
    "- argument('--num-speculative-tokens', type=int, default=EngineArgs.num_speculative_tokens, help='The number of speculative tokens to sample from the draft model in speculative decoding.')\n",
    "- argument('--speculative-max-model-len', type=int, default=EngineArgs.speculative_max_model_len, help='The maximum sequence length supported by the draft model. Sequences over this length will skip speculation.')\n",
    "- argument('--speculative-disable-by-batch-size', type=int, default=EngineArgs.speculative_disable_by_batch_size, help='Disable speculative decoding for new incoming requests if the number of enqueue requests is larger than this value.')\n",
    "- argument('--ngram-prompt-lookup-max', type=int, default=EngineArgs.ngram_prompt_lookup_max, help='Max size of window for ngram prompt lookup in speculative decoding.')\n",
    "- argument('--ngram-prompt-lookup-min', type=int, default=EngineArgs.ngram_prompt_lookup_min, help='Min size of window for ngram prompt lookup in speculative decoding.')\n",
    "\n",
    "> LoRA related configs\n",
    "\n",
    "- argument('--enable-lora', action='store_true', help='If True, enable handling of LoRA adapters.')\n",
    "- argument('--max-loras', type=int, default=EngineArgs.max_loras, help='Max number of LoRAs in a single batch.')\n",
    "- argument('--max-lora-rank', type=int, default=EngineArgs.max_lora_rank, help='Max LoRA rank.')\n",
    "- argument('--lora-extra-vocab-size', type=int, default=EngineArgs.lora_extra_vocab_size, help=('Maximum size of extra vocabulary that can be present in a LoRA adapter (added to the base model vocabulary).'))\n",
    "- argument('--lora-dtype', type=str, default=EngineArgs.lora_dtype, choices=['auto', 'float16', 'bfloat16', 'float32'], help=('Data type for LoRA. If auto, will default to base model dtype.'))\n",
    "- argument('--long-lora-scaling-factors', type=nullable_str, default=EngineArgs.long_lora_scaling_factors, help=('Specify multiple scaling factors (which can be different from base model scaling factor - see eg. Long LoRA) to allow for multiple LoRA adapters trained with those scaling factors to be used at the same time. If not specified, only adapters trained with the base model scaling factor are allowed.'))\n",
    "- argument('--max-cpu-loras', type=int, default=EngineArgs.max_cpu_loras, help=('Maximum number of LoRAs to store in CPU memory. Must be >= than max_num_seqs. Defaults to max_num_seqs.'))\n",
    "- argument('--fully-sharded-loras', action='store_true', help=('By default, only half of the LoRA computation is sharded with tensor parallelism. Enabling this will use the fully sharded layers. At high sequence length, max rank or tensor parallel size, this is likely faster.'))\n",
    "\n",
    "> Scheduler, batches, pools, stats\n",
    "\n",
    "- argument('--scheduler-delay-factor', type=float, default=EngineArgs.scheduler_delay_factor, help='Apply a delay (of delay factor multiplied by previous prompt latency) before scheduling next prompt.')\n",
    "- argument('--enable-chunked-prefill', action='store_true', help='If set, the prefill requests can be chunked based on the max_num_batched_tokens.')\n",
    "- argument('--max-num-batched-tokens', type=int, default=EngineArgs.max_num_batched_tokens, help='Maximum number of batched tokens per iteration.')\n",
    "- argument('--max-num-seqs', type=int, default=EngineArgs.max_num_seqs, help='Maximum number of sequences per iteration.')\n",
    "- argument('--max-logprobs', type=int, default=EngineArgs.max_logprobs, help=('Max number of log probs to return logprobs is specified in SamplingParams.'))\n",
    "- argument('--tokenizer-pool-size', type=int, default=EngineArgs.tokenizer_pool_size, help='Size of tokenizer pool to use for asynchronous tokenization. If 0, will use synchronous tokenization.')\n",
    "- argument('--tokenizer-pool-type', type=str, default=EngineArgs.tokenizer_pool_type, help='Type of tokenizer pool to use for asynchronous tokenization. Ignored if tokenizer_pool_size is 0.')\n",
    "- argument('--tokenizer-pool-extra-config', type=nullable_str, default=EngineArgs.tokenizer_pool_extra_config, help='Extra config for tokenizer pool. This should be a JSON string that will be parsed into a dictionary. Ignored if tokenizer_pool_size is 0.')\n",
    "- argument('--disable-log-stats', action='store_true', help='Disable logging statistics.')\n",
    "\n",
    "> KV cache arguments\n",
    "\n",
    "- argument('--block-size', type=int, default=EngineArgs.block_size, choices=[8, 16, 32], help='Token block size for contiguous chunks of tokens.')\n",
    "- argument('--enable-prefix-caching', action='store_true', help='Enables automatic prefix caching.')\n",
    "- argument('--use-v2-block-manager', action='store_true', help='Use BlockSpaceMangerV2.')\n",
    "- argument('--num-lookahead-slots', type=int, default=EngineArgs.num_lookahead_slots, help='Experimental scheduling config necessary for speculative decoding. This will be replaced by speculative config in the future; it is present to enable correctness tests until then.')\n",
    "\n",
    "> Related to Vision-language models such as llava\n",
    "\n",
    "- argument('--image-input-type', type=nullable_str, default=None, choices=[t.name.lower() for t in VisionLanguageConfig.ImageInputType], help=('The image input type passed into vLLM. Should be one of \"pixel_values\" or \"image_features\".'))\n",
    "- argument('--image-token-id',type=int, default=None, help=('Input id for image token.'))\n",
    "- argument('--image-input-shape', type=nullable_str, default=None, help=('The biggest image input shape (worst for memory footprint) given an input type. Only used for vLLM\\'s profile_run.'))\n",
    "- argument('--image-feature-size', type=int, default=None, help=('The image feature size along the context dimension.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78907ba-d9cd-45ae-8a61-30f058dae7e9",
   "metadata": {},
   "source": [
    "## class EngineArgs\n",
    "\n",
    "Arguments for vLLM engine.\n",
    "\n",
    "```python\n",
    "    model: str\n",
    "    served_model_name: Optional[Union[List[str]]] = None\n",
    "    tokenizer: Optional[str] = None\n",
    "    skip_tokenizer_init: bool = False\n",
    "    tokenizer_mode: str = 'auto'\n",
    "    trust_remote_code: bool = False\n",
    "    download_dir: Optional[str] = None\n",
    "    load_format: str = 'auto'\n",
    "    dtype: str = 'auto'\n",
    "    kv_cache_dtype: str = 'auto'\n",
    "    quantization_param_path: Optional[str] = None\n",
    "    seed: int = 0\n",
    "    max_model_len: Optional[int] = None\n",
    "    worker_use_ray: bool = False\n",
    "    distributed_executor_backend: Optional[str] = None\n",
    "    pipeline_parallel_size: int = 1\n",
    "    tensor_parallel_size: int = 1\n",
    "    max_parallel_loading_workers: Optional[int] = None\n",
    "    block_size: int = 16\n",
    "    enable_prefix_caching: bool = False\n",
    "    use_v2_block_manager: bool = False\n",
    "    swap_space: int = 4  # GiB\n",
    "    gpu_memory_utilization: float = 0.90\n",
    "    max_num_batched_tokens: Optional[int] = None\n",
    "    max_num_seqs: int = 256\n",
    "    max_logprobs: int = 5  # OpenAI default value\n",
    "    disable_log_stats: bool = False\n",
    "    revision: Optional[str] = None\n",
    "    code_revision: Optional[str] = None\n",
    "    rope_scaling: Optional[dict] = None\n",
    "    tokenizer_revision: Optional[str] = None\n",
    "    quantization: Optional[str] = None\n",
    "    enforce_eager: bool = False\n",
    "    max_context_len_to_capture: Optional[int] = None\n",
    "    max_seq_len_to_capture: int = 8192\n",
    "    disable_custom_all_reduce: bool = False\n",
    "    tokenizer_pool_size: int = 0\n",
    "    tokenizer_pool_type: str = \"ray\"\n",
    "    tokenizer_pool_extra_config: Optional[dict] = None\n",
    "    enable_lora: bool = False\n",
    "    max_loras: int = 1\n",
    "    max_lora_rank: int = 16\n",
    "    fully_sharded_loras: bool = False\n",
    "    lora_extra_vocab_size: int = 256\n",
    "    long_lora_scaling_factors: Optional[Tuple[float]] = None\n",
    "    lora_dtype = 'auto'\n",
    "    max_cpu_loras: Optional[int] = None\n",
    "    device: str = 'auto'\n",
    "    ray_workers_use_nsight: bool = False\n",
    "    num_gpu_blocks_override: Optional[int] = None\n",
    "    num_lookahead_slots: int = 0\n",
    "    model_loader_extra_config: Optional[dict] = None\n",
    "\n",
    "    # Related to Vision-language models such as llava\n",
    "    image_input_type: Optional[str] = None\n",
    "    image_token_id: Optional[int] = None\n",
    "    image_input_shape: Optional[str] = None\n",
    "    image_feature_size: Optional[int] = None\n",
    "    scheduler_delay_factor: float = 0.0\n",
    "    enable_chunked_prefill: bool = False\n",
    "\n",
    "    guided_decoding_backend: str = 'outlines'\n",
    "    # Speculative decoding configuration.\n",
    "    speculative_model: Optional[str] = None\n",
    "    num_speculative_tokens: Optional[int] = None\n",
    "    speculative_max_model_len: Optional[int] = None\n",
    "    speculative_disable_by_batch_size: Optional[int] = None\n",
    "    ngram_prompt_lookup_max: Optional[int] = None\n",
    "    ngram_prompt_lookup_min: Optional[int] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf161f-a8e5-48f4-a9e4-72689afafecd",
   "metadata": {},
   "source": [
    "## class LLMEngine\n",
    "\n",
    "An LLM engine that receives requests and generates texts.\n",
    "\n",
    "This is the main class for the vLLM engine. It receives requests\n",
    "from clients and generates texts from the LLM. It includes a tokenizer, a\n",
    "language model (possibly distributed across multiple GPUs), and GPU memory\n",
    "space allocated for intermediate states (aka KV cache). This class utilizes\n",
    "iteration-level scheduling and efficient memory management to maximize the\n",
    "serving throughput.\n",
    "\n",
    "The `LLM` class wraps this class for offline batched inference and the\n",
    "`AsyncLLMEngine` class wraps this class for online serving.\n",
    "\n",
    "NOTE: The config arguments are derived from the `EngineArgs` class. For the\n",
    "comprehensive list of arguments, see `EngineArgs`.\n",
    "\n",
    "### from_engine_args()\n",
    "    \n",
    "Creates an LLM engine from the engine arguments.\n",
    "\n",
    "- engine_args: EngineArgs,\n",
    "- usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n",
    "        \n",
    "1. Create the engine configs.\n",
    "2. Initialize the cluster and specify the executor class.\n",
    "3. Create the LLM engine.\n",
    "\n",
    "### __init__()\n",
    "\n",
    "- model_config: The configuration related to the LLM model.\n",
    "- cache_config: The configuration related to the KV cache memory management.\n",
    "- parallel_config: The configuration related to distributed execution.\n",
    "- scheduler_config: The configuration related to the request scheduler.\n",
    "- device_config: The configuration related to the device.\n",
    "- lora_config (Optional): The configuration related to serving multi-LoRA.\n",
    "- vision_language_config (Optional): The configuration related to vision language models.\n",
    "- speculative_config (Optional): The configuration related to speculative decoding.\n",
    "- executor_class: The model executor class for managing distributed execution.\n",
    "- log_stats: Whether to log statistics.\n",
    "- usage_context: Specified entry point, used for usage info collection\n",
    "\n",
    "Types\n",
    "\n",
    "- model_config: ModelConfig,\n",
    "- cache_config: CacheConfig,\n",
    "- parallel_config: ParallelConfig,\n",
    "- scheduler_config: SchedulerConfig,\n",
    "- device_config: DeviceConfig,\n",
    "- load_config: LoadConfig,\n",
    "- lora_config: Optional[LoRAConfig],\n",
    "- vision_language_config: Optional[VisionLanguageConfig],\n",
    "- speculative_config: Optional[SpeculativeConfig],\n",
    "- decoding_config: Optional[DecodingConfig],\n",
    "- executor_class: Type[ExecutorBase],\n",
    "- log_stats: bool,\n",
    "- usage_context: UsageContext = UsageContext.ENGINE_CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bde525-f0e7-4fd2-9ebc-35d078de21db",
   "metadata": {},
   "source": [
    "## class AsyncLLMEngine\n",
    "\n",
    "An asynchronous wrapper for LLMEngine.\n",
    "\n",
    "This class is used to wrap the LLMEngine class to make it asynchronous. It\n",
    "uses asyncio to create a background loop that keeps processing incoming\n",
    "requests. The LLMEngine is kicked by the generate method when there\n",
    "are requests in the waiting queue. The generate method yields the outputs\n",
    "from the LLMEngine to the caller.\n",
    "\n",
    "NOTE: For the comprehensive list of arguments, see `LLMEngine`.\n",
    "\n",
    "### __init__()\n",
    "\n",
    "- worker_use_ray: Whether to use Ray for model workers. Required for distributed execution. Should be the same as `parallel_config.worker_use_ray`.\n",
    "- engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the async frontend will be executed in a separate process as the model workers.\n",
    "- log_requests: Whether to log the requests.\n",
    "- max_log_len: Maximum number of prompt characters or prompt ID numbers being printed in log.\n",
    "- start_engine_loop: If True, the background task to run the engine will be automatically started in the generate call.\n",
    "- *args: Arguments for LLMEngine.\n",
    "- *kwargs: Arguments for LLMEngine.\n",
    "\n",
    "Types \n",
    "\n",
    "- worker_use_ray: bool,\n",
    "- engine_use_ray: bool,\n",
    "- log_requests: bool = True,\n",
    "- max_log_len: Optional[int] = None,\n",
    "- start_engine_loop: bool = True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80567add-c1b0-4ddd-9cfd-19e5e2bcc35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
