{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbf161f-a8e5-48f4-a9e4-72689afafecd",
   "metadata": {},
   "source": [
    "## class LLMEngine\n",
    "\n",
    "An LLM engine that receives requests and generates texts.\n",
    "\n",
    "This is the main class for the vLLM engine. It receives requests\n",
    "from clients and generates texts from the LLM. It includes a tokenizer, a\n",
    "language model (possibly distributed across multiple GPUs), and GPU memory\n",
    "space allocated for intermediate states (aka KV cache). This class utilizes\n",
    "iteration-level scheduling and efficient memory management to maximize the\n",
    "serving throughput.\n",
    "\n",
    "The `LLM` class wraps this class for offline batched inference and the\n",
    "`AsyncLLMEngine` class wraps this class for online serving.\n",
    "\n",
    "NOTE: The config arguments are derived from the `EngineArgs` class. For the\n",
    "comprehensive list of arguments, see `EngineArgs`.\n",
    "\n",
    "### def from_engine_args()\n",
    "    \n",
    "Creates an LLM engine from the engine arguments.\n",
    "\n",
    "- engine_args: EngineArgs,\n",
    "- usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n",
    "        \n",
    "1. Create the engine configs.\n",
    "2. Initialize the cluster and specify the executor class.\n",
    "3. Create the LLM engine.\n",
    "\n",
    "### def __init__()\n",
    "\n",
    "- model_config: The configuration related to the LLM model.\n",
    "- cache_config: The configuration related to the KV cache memory management.\n",
    "- parallel_config: The configuration related to distributed execution.\n",
    "- scheduler_config: The configuration related to the request scheduler.\n",
    "- device_config: The configuration related to the device.\n",
    "- lora_config (Optional): The configuration related to serving multi-LoRA.\n",
    "- vision_language_config (Optional): The configuration related to vision language models.\n",
    "- speculative_config (Optional): The configuration related to speculative decoding.\n",
    "- executor_class: The model executor class for managing distributed execution.\n",
    "- log_stats: Whether to log statistics.\n",
    "- usage_context: Specified entry point, used for usage info collection\n",
    "\n",
    "- model_config: ModelConfig,\n",
    "- cache_config: CacheConfig,\n",
    "- parallel_config: ParallelConfig,\n",
    "- scheduler_config: SchedulerConfig,\n",
    "- device_config: DeviceConfig,\n",
    "- load_config: LoadConfig,\n",
    "- lora_config: Optional[LoRAConfig],\n",
    "- vision_language_config: Optional[VisionLanguageConfig],\n",
    "- speculative_config: Optional[SpeculativeConfig],\n",
    "- decoding_config: Optional[DecodingConfig],\n",
    "- executor_class: Type[ExecutorBase],\n",
    "- log_stats: bool,\n",
    "- usage_context: UsageContext = UsageContext.ENGINE_CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf836ce-f0c5-4872-b6c0-015578d30b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
