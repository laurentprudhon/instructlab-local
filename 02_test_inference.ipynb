{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79e0989-feba-4f18-a754-856427aca303",
   "metadata": {},
   "source": [
    "# Instructlab local - 02 Test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f29e0-f67d-4920-9510-7646222082fa",
   "metadata": {},
   "source": [
    "## 1. VLLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cec0a20-5a4b-4b59-b1cb-794470b6e223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:46:44.041434Z",
     "iopub.status.busy": "2024-06-01T21:46:44.040120Z",
     "iopub.status.idle": "2024-06-01T21:46:44.046268Z",
     "shell.execute_reply": "2024-06-01T21:46:44.045406Z",
     "shell.execute_reply.started": "2024-06-01T21:46:44.041383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "\"mistral\":\"mistralai/Mistral-7B-v0.3\",\n",
    "\"mistral-instruct\":\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "\"llama3\":\"meta-llama/Meta-Llama-3-8B\",\n",
    "\"llama3-instruct\":\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "\"phi3-mini\":\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "\"phi3-small\":\"microsoft/Phi-3-small-8k-instruct\",\n",
    "\"mixtral-q3\":\"mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ\",\n",
    "\"mixtral-q2\":\"mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29a6b4-09aa-4217-8222-8779195f456c",
   "metadata": {},
   "source": [
    "**IMPORTANT: always set the local download cache directory explicitly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8eb5493-b4a5-4ef0-b989-530bae9df9b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:46:44.788315Z",
     "iopub.status.busy": "2024-06-01T21:46:44.787310Z",
     "iopub.status.idle": "2024-06-01T21:46:44.793916Z",
     "shell.execute_reply": "2024-06-01T21:46:44.793055Z",
     "shell.execute_reply.started": "2024-06-01T21:46:44.788263Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download cache dir: /models/huggingface/transformers OK\n"
     ]
    }
   ],
   "source": [
    "DOWNLOAD_CACHE_DIR = \"/models/huggingface/transformers\"\n",
    "\n",
    "from pathlib import Path\n",
    "print(f\"Download cache dir: {DOWNLOAD_CACHE_DIR} {('OK' if Path(DOWNLOAD_CACHE_DIR).exists() else 'KO')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0fc852-9777-46e4-87bb-b1c7e2512cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T16:10:43.923375Z",
     "iopub.status.busy": "2024-05-25T16:10:43.920040Z",
     "iopub.status.idle": "2024-05-25T16:10:43.974118Z",
     "shell.execute_reply": "2024-05-25T16:10:43.973099Z",
     "shell.execute_reply.started": "2024-05-25T16:10:43.922598Z"
    },
    "tags": []
   },
   "source": [
    "### 1.1 Mistral 7B instruct v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c932f200-a455-4180-9055-ca972b699b71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:46:46.349702Z",
     "iopub.status.busy": "2024-06-01T21:46:46.348866Z",
     "iopub.status.idle": "2024-06-01T21:46:59.960329Z",
     "shell.execute_reply": "2024-06-01T21:46:59.959917Z",
     "shell.execute_reply.started": "2024-06-01T21:46:46.349665Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/instructlab-local/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model mistralai/Mistral-7B-Instruct-v0.3\n",
      "INFO 06-01 21:46:47 config.py:390] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\n",
      "INFO 06-01 21:46:47 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/models/huggingface/transformers', load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/instructlab-local/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-01 21:46:48 utils.py:451] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 06-01 21:46:48 selector.py:130] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "INFO 06-01 21:46:48 selector.py:51] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-01 21:46:49 selector.py:130] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "INFO 06-01 21:46:49 selector.py:51] Using XFormers backend.\n",
      "INFO 06-01 21:46:49 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-01 21:46:52 model_runner.py:146] Loading model weights took 13.5083 GB\n",
      "INFO 06-01 21:46:55 gpu_executor.py:83] # GPU blocks: 6082, # CPU blocks: 4096\n",
      "INFO 06-01 21:46:55 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-01 21:46:55 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-01 21:46:59 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_name = models[\"mistral-instruct\"]\n",
    "\n",
    "print(f\"Loading model {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = LLM(model_name, kv_cache_dtype=\"fp8\", gpu_memory_utilization=0.99, download_dir=DOWNLOAD_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b74b0-f4cc-4f04-ab87-535a6cac1c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install mistral_common==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bdb123a-ca7c-491f-9ffd-5231b7a8f0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:46:59.961282Z",
     "iopub.status.busy": "2024-06-01T21:46:59.961096Z",
     "iopub.status.idle": "2024-06-01T21:47:00.082021Z",
     "shell.execute_reply": "2024-06-01T21:47:00.081487Z",
     "shell.execute_reply.started": "2024-06-01T21:46:59.961271Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/models/huggingface/transformers/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/83e9aa141f2e28c82232fea5325f54edf17c43de/tokenizer.model.v3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.utils.hub import cached_file \n",
    "\n",
    "tokenizer_model_file = cached_file(model_name, \"tokenizer.model.v3\", cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "tokenizer_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df31fb71-7374-4f22-a17a-52983ba3bc94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:04.581044Z",
     "iopub.status.busy": "2024-06-01T21:47:04.579924Z",
     "iopub.status.idle": "2024-06-01T21:47:04.631835Z",
     "shell.execute_reply": "2024-06-01T21:47:04.631338Z",
     "shell.execute_reply.started": "2024-06-01T21:47:04.580993Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "tokenizer = MistralTokenizer.from_file(tokenizer_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84a83a5-2bcf-43c2-a4b6-29479ecfbe73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:04.999597Z",
     "iopub.status.busy": "2024-06-01T21:47:04.999033Z",
     "iopub.status.idle": "2024-06-01T21:47:05.016864Z",
     "shell.execute_reply": "2024-06-01T21:47:05.016387Z",
     "shell.execute_reply.started": "2024-06-01T21:47:04.999563Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST]▁quelles▁sont▁les▁principales▁ouvertures▁des▁échecs▁?[/INST]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mistral_common.protocol.instruct.messages import SystemMessage, UserMessage, AssistantMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=\"quelles sont les principales ouvertures des échecs ?\")])\n",
    "\n",
    "tokens = tokenizer.encode_chat_completion(completion_request)\n",
    "tokens.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c0b757-c114-434c-876d-0b2d8cd0d856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:05.519864Z",
     "iopub.status.busy": "2024-06-01T21:47:05.519044Z",
     "iopub.status.idle": "2024-06-01T21:47:05.528911Z",
     "shell.execute_reply": "2024-06-01T21:47:05.528473Z",
     "shell.execute_reply.started": "2024-06-01T21:47:05.519835Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "def vllm_stream_output(\n",
    "    llm: LLM,\n",
    "    prompt: str,\n",
    "    sampling_params: Optional[SamplingParams] = None,\n",
    "    lora_request: Optional[LoRARequest] = None\n",
    "    ):\n",
    "\n",
    "    if sampling_params is None:\n",
    "        # Use default sampling params.\n",
    "        sampling_params = SamplingParams()\n",
    "\n",
    "    llm._validate_and_add_requests(\n",
    "        inputs=prompt,\n",
    "        params=sampling_params,\n",
    "        lora_request=lora_request,\n",
    "    )\n",
    "\n",
    "    while llm.llm_engine.has_unfinished_requests():\n",
    "        step_outputs = llm.llm_engine.step()\n",
    "        output = step_outputs[0]\n",
    "        yield output.outputs[0].text\n",
    "        if output.finished:\n",
    "            yield output\n",
    "\n",
    "def vllm_display_output(\n",
    "    llm: LLM,\n",
    "    prompt: str,\n",
    "    sampling_params: Optional[SamplingParams] = None,\n",
    "    lora_request: Optional[LoRARequest] = None\n",
    "    ):\n",
    "          \n",
    "    last_length = 0\n",
    "    for result in vllm_stream_output(llm, prompt, sampling_params=sampling_params, lora_request=lora_request):\n",
    "        if isinstance(result,str):\n",
    "            print(result[last_length:], end=\"\", flush=True)\n",
    "            last_length = len(result)\n",
    "        else:\n",
    "            print()\n",
    "            print(\"--------------------------\")\n",
    "            input_tokens = len(result.prompt_token_ids)\n",
    "            output_tokens = len(result.outputs[0].token_ids)\n",
    "            total_time = result.metrics.finished_time-result.metrics.arrival_time\n",
    "            time_to_first_token = result.metrics.first_token_time-result.metrics.arrival_time\n",
    "            tokens_per_sec = (output_tokens-1)/(total_time-time_to_first_token)\n",
    "            print(f\"{output_tokens} tokens generated in {total_time:.2f} sec | prompt: {input_tokens:.2f} tokens | time to first token: {time_to_first_token:.2f} sec | throughput: {tokens_per_sec:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22823e51-7379-42fc-a8ce-a70b41f313a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:08.096536Z",
     "iopub.status.busy": "2024-06-01T21:47:08.095296Z",
     "iopub.status.idle": "2024-06-01T21:47:15.854989Z",
     "shell.execute_reply": "2024-06-01T21:47:15.854543Z",
     "shell.execute_reply.started": "2024-06-01T21:47:08.096476Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Les principales ouvertures d'échecs, classées par ordre de popularité, sont :\n",
      "\n",
      "1. Ouverture française (ECO C00) : 1.e4 e6 2.d4 d5\n",
      "2. Défense sicilienne (ECO B00-B99) : 1.e4 c5\n",
      "3. Ouverture du roi (ECO A00-A99) : 1.d4\n",
      "4. Défense indienne (ECO E00-E99) : 1.d4 Cf6\n",
      "5. Défense gréco (ECO D00-D99) : 1.e4 e5\n",
      "6. Défense caro-kann (ECO B10-B99) : 1.e4 c6\n",
      "7. Ouverture scandinave (ECO A10-A99) : 1.e4 d5 2.exd5 Cf6\n",
      "8. Ouverture espagnole (ECO C80-C99) : 1.e4 e5 2.Cf3 Cc6\n",
      "9. Défense sicilienne variante Najdorf (ECO B97) : 1.e4 c5 2.d4 cxd4 3.Cc3 a6 4.Cf3 Cf6 5.dxc5\n",
      "10. Défense sicilienne variante Scheveningen (ECO B84) : 1.e4 c5 2.Nf3 d6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 a6 6.f3.\n",
      "--------------------------\n",
      "403 tokens generated in 7.75 sec | prompt: 18.00 tokens | time to first token: 0.27 sec | throughput: 53.72 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 1024\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=max_tokens)\n",
    "\n",
    "vllm_display_output(llm, tokens.text,sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09c1a46-9d3f-4c25-813c-7c9b45025f9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T20:08:12.724697Z",
     "iopub.status.busy": "2024-06-01T20:08:12.724229Z",
     "iopub.status.idle": "2024-06-01T20:08:31.427637Z",
     "shell.execute_reply": "2024-06-01T20:08:31.427149Z",
     "shell.execute_reply.started": "2024-06-01T20:08:12.724670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.70s/it, Generation Speed: 54.77 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Les principales ouvertures d'échecs sont les suivantes :\n",
      "\n",
      "1. Ouverture française (ECO C00-C99) : 1. e4 e6\n",
      "2. Ouverture anglaise (ECO A00-A99) : 1. c4\n",
      "3. Défense sicilienne (ECO B00-B99) : 1. e4 c5\n",
      "4. Défense indienne (ECO E00-E99) : 1. d4\n",
      "5. Défense sicilienne, variante Najdorf (ECO B97) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Cf6 5. Nc3 a6 6. Rd1\n",
      "6. Défense sicilienne, variante Scheveningen (ECO B84) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2\n",
      "7. Ouverture du roi (ECO D00-D99) : 1. d4 d5\n",
      "8. Défense russe (ECO E10-E99) : 1. d4 d5 2. c4 c6\n",
      "9. Défense gréco (ECO D05-D09) : 1. d4 d5 2. c4 c6 3. Cf3\n",
      "10. Défense caro-kann (ECO B10-B99) : 1. e4 c6 2. d4 d5\n",
      "11. Défense benoni (ECO A60-A69) : 1. d4 c5 2. d5 e6 3. c4\n",
      "12. Défense scandinave (ECO B03) : 1. e4 d5 2. exd5 Nf6 3. Nc3\n",
      "13. Défense danoise (ECO B04) : 1. e4 e5 2. d4 exd4 3. Qxd4 Nc6\n",
      "14. Défense sicilienne, variante Taimanov (ECO B88) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Bd3\n",
      "15. Défense sicilienne, variante Scheveningen, variante Richter-Rauzer (ECO B95) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Bg5 h6 7. Bxf6\n",
      "16. Défense sicilienne, variante Dragon (ECO B09) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 g6\n",
      "17. Défense sicilienne, variante Najdorf, variante 6. Be2 (ECO B97) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2\n",
      "18. Défense sicilienne, variante Najdorf, variante 6. Bd3 (ECO B97) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Bd3\n",
      "19. Défense sicilienne, variante Najdorf, variante 6. Be2, variante Bb4+ (ECO B97) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2 b5 7. Bb4+ c6 8. d5\n",
      "20. Défense sicilienne, variante Najdorf, variante\n",
      "-------------------------\n",
      "1024 tokens generated in 18.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "requests_results = llm.generate(tokens.text, sampling_params=sampling_params)\n",
    "result = requests_results[0]\n",
    "output = result.outputs[0]\n",
    "print(output.text)\n",
    "print(\"-------------------------\")\n",
    "print(f\"{len(output.token_ids)} tokens generated in {result.metrics.finished_time-result.metrics.arrival_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0a8156-4951-4a28-aaab-9170adfcad6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:23.959367Z",
     "iopub.status.busy": "2024-06-01T21:47:23.958566Z",
     "iopub.status.idle": "2024-06-01T21:47:23.966048Z",
     "shell.execute_reply": "2024-06-01T21:47:23.965571Z",
     "shell.execute_reply.started": "2024-06-01T21:47:23.959333Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This port used by gradio apps in the container: 7860\n",
      "Will be exposed at the following url on the virtual machine: /notebooks/gradio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/notebooks/gradio/\" target=\"_blank\">Click here to access the gradio app</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "gradio_server_port = int(os.environ.get('GRADIO_PORT'))\n",
    "gradio_root_path = os.environ.get('GRADIO_BASE_URL')\n",
    "\n",
    "print(f\"This port used by gradio apps in the container: {gradio_server_port}\")\n",
    "print(f\"Will be exposed at the following url on the virtual machine: {gradio_root_path}\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(f'<a href=\"{gradio_root_path}/\" target=\"_blank\">Click here to access the gradio app</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0122029-aa1c-475b-9791-86da2e092062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:27.337652Z",
     "iopub.status.busy": "2024-06-01T21:47:27.336721Z",
     "iopub.status.idle": "2024-06-01T21:47:28.452177Z",
     "shell.execute_reply": "2024-06-01T21:47:28.451590Z",
     "shell.execute_reply.started": "2024-06-01T21:47:27.337598Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "69 tokens generated in 1.58 sec | prompt: 49.00 tokens | time to first token: 0.34 sec | throughput: 54.76 tokens/sec\n",
      "\n",
      "--------------------------\n",
      "121 tokens generated in 2.47 sec | prompt: 129.00 tokens | time to first token: 0.31 sec | throughput: 55.61 tokens/sec\n",
      "\n",
      "--------------------------\n",
      "195 tokens generated in 3.74 sec | prompt: 267.00 tokens | time to first token: 0.23 sec | throughput: 55.21 tokens/sec\n",
      "\n",
      "--------------------------\n",
      "195 tokens generated in 3.95 sec | prompt: 277.00 tokens | time to first token: 0.33 sec | throughput: 53.73 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "system_prompt = \"Tu es MonIA, un assistant expert qui essaie toujours d'être concis, exact, utile et poli.\"\n",
    "\n",
    "def apply_chat_template(system_prompt, history, message):\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    for turn in history:\n",
    "        messages.append(UserMessage(content=turn[0]))\n",
    "        messages.append(AssistantMessage(content=turn[1]))\n",
    "    messages.append(UserMessage(content=message))\n",
    "    completion_request = ChatCompletionRequest(messages=messages)\n",
    "    tokens = tokenizer.encode_chat_completion(completion_request)\n",
    "    return tokens.text\n",
    "\n",
    "def process_message(message, history, system_prompt):\n",
    "    prompt = apply_chat_template(system_prompt, history, message)\n",
    "    for result in vllm_stream_output(llm, prompt, sampling_params=sampling_params):\n",
    "        if isinstance(result,str):\n",
    "            yield result\n",
    "        else:\n",
    "            print()\n",
    "            print(\"--------------------------\")\n",
    "            input_tokens = len(result.prompt_token_ids)\n",
    "            output_tokens = len(result.outputs[0].token_ids)\n",
    "            total_time = result.metrics.finished_time-result.metrics.arrival_time\n",
    "            time_to_first_token = result.metrics.first_token_time-result.metrics.arrival_time\n",
    "            tokens_per_sec = (output_tokens-1)/(total_time-time_to_first_token)\n",
    "            print(f\"{output_tokens} tokens generated in {total_time:.2f} sec | prompt: {input_tokens:.2f} tokens | time to first token: {time_to_first_token:.2f} sec | throughput: {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "\n",
    "demo = gr.ChatInterface(process_message, title=model_name, additional_inputs=[\n",
    "        gr.Textbox(system_prompt, label=\"System Prompt\")\n",
    "    ], additional_inputs_accordion=\"Configuration\").queue()\n",
    "demo.launch(inline=False, share=False, server_name=\"0.0.0.0\", server_port=gradio_server_port, root_path=gradio_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "147bb51e-36e3-4a7a-aa92-9913bfd90ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:44:07.712133Z",
     "iopub.status.busy": "2024-06-01T21:44:07.710608Z",
     "iopub.status.idle": "2024-06-01T21:44:07.720868Z",
     "shell.execute_reply": "2024-06-01T21:44:07.720453Z",
     "shell.execute_reply.started": "2024-06-01T21:44:07.712074Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd79f3-5570-404e-b693-c209bf943c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "prompt = \"Quels sont les avantages du Crédit Mutuel ?\"\n",
    "\n",
    "# System prompt\n",
    "messages = [\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "],\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quels sont les avantages du Crédit Agricole ?\"}\n",
    "],\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quels sont les avantages de la Société Générale ?\"}\n",
    "],\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quels sont les avantages de la BNP ?\"}\n",
    "],\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quels sont les avantages de la Banque populaire ?\"}\n",
    "],\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quels sont les avantages de la Caise d'épargne ?\"}\n",
    "]\n",
    "]\n",
    "\n",
    "# Generate outputs\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "start_time = time.time()  # Record the start time\n",
    "outputs = llm.generate(text, sampling_params)\n",
    "end_time = time.time()  # Record the end time\n",
    "    \n",
    "# Print the outputs.\n",
    "tokenscount = 0\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    tokenscount = tokenscount + len(output.outputs[0].token_ids)\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "    \n",
    "print(f\"Performance: {int(tokenscount/(end_time-start_time))} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbd601-3869-4cf9-a478-b1e4cbcd2b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructlab-local",
   "language": "python",
   "name": "instructlab-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
