{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4f2c52-5a4e-43d6-aa41-4d7ac8b34fb2",
   "metadata": {},
   "source": [
    "# Instructlab local - 01 Prepare environment\n",
    "\n",
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b38a0a8-cd35-4b9a-ae4c-f109464ce298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T08:29:40.559524Z",
     "iopub.status.busy": "2024-05-25T08:29:40.558647Z",
     "iopub.status.idle": "2024-05-25T08:29:40.693111Z",
     "shell.execute_reply": "2024-05-25T08:29:40.692454Z",
     "shell.execute_reply.started": "2024-05-25T08:29:40.559506Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, memory.total [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total --format csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3eae68e-e9d7-43df-add9-beded295d8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T08:32:25.868135Z",
     "iopub.status.busy": "2024-05-25T08:32:25.867711Z",
     "iopub.status.idle": "2024-05-25T08:32:25.949395Z",
     "shell.execute_reply": "2024-05-25T08:32:25.948976Z",
     "shell.execute_reply.started": "2024-05-25T08:32:25.868116Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| NVIDIA-SMI 550.76.01              Driver Version: 552.44         CUDA Version: 12.4     |\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(\"nvidia-smi | grep 'CUDA Version'\", shell=True, capture_output=True, text=True)\n",
    "result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9362ae5d-fa87-4d6f-824e-339d2d7dab31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:19:34.070822Z",
     "iopub.status.busy": "2024-06-02T06:19:34.070409Z",
     "iopub.status.idle": "2024-06-02T06:19:34.076123Z",
     "shell.execute_reply": "2024-06-02T06:19:34.074903Z",
     "shell.execute_reply.started": "2024-06-02T06:19:34.070795Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd6d9a-fa0e-4a21-999d-aab71b234a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torch==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de7967e-c770-4229-a972-7811c1d1262e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:19:30.073281Z",
     "iopub.status.busy": "2024-06-02T06:19:30.072836Z",
     "iopub.status.idle": "2024-06-02T06:19:30.081378Z",
     "shell.execute_reply": "2024-06-02T06:19:30.080960Z",
     "shell.execute_reply.started": "2024-06-02T06:19:30.073253Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e814904e-0279-4bdb-b754-1a09f17a4961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers==4.41.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa0afeb-54b1-417c-af01-237c83b245a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:27:00.937375Z",
     "iopub.status.busy": "2024-05-25T13:27:00.935618Z",
     "iopub.status.idle": "2024-05-25T13:27:00.945375Z",
     "shell.execute_reply": "2024-05-25T13:27:00.944933Z",
     "shell.execute_reply.started": "2024-05-25T13:27:00.937331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.41.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa295c95-58b2-4245-87d0-2bebb9f26d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install accelerate==0.30.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e98156-f674-4cba-a5e3-38be68f030cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:27:03.853477Z",
     "iopub.status.busy": "2024-05-25T13:27:03.852930Z",
     "iopub.status.idle": "2024-05-25T13:27:03.859330Z",
     "shell.execute_reply": "2024-05-25T13:27:03.858702Z",
     "shell.execute_reply.started": "2024-05-25T13:27:03.853455Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.30.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('accelerate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78fa79-57bb-4c5a-8f59-0046074a867c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install flash-attn==2.5.8 --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ff6406-e726-4b45-8171-7a8fa3fe8151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:28:34.432938Z",
     "iopub.status.busy": "2024-05-25T13:28:34.432001Z",
     "iopub.status.idle": "2024-05-25T13:28:34.442435Z",
     "shell.execute_reply": "2024-05-25T13:28:34.441380Z",
     "shell.execute_reply.started": "2024-05-25T13:28:34.432894Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.8'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('flash_attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1116b-2c3a-4153-8f0f-d99a638c9c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install vllm==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20798362-287e-4fbc-a0d0-ce08f0c3089c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T17:49:39.229065Z",
     "iopub.status.busy": "2024-06-01T17:49:39.228293Z",
     "iopub.status.idle": "2024-06-01T17:49:39.236765Z",
     "shell.execute_reply": "2024-06-01T17:49:39.235889Z",
     "shell.execute_reply.started": "2024-06-01T17:49:39.229034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cbd4e-b7d8-476e-a338-76f3413062ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install gradio==4.31.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cf7bb6a-ce7d-40ba-b4e1-400e65d41864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T16:06:40.104356Z",
     "iopub.status.busy": "2024-05-25T16:06:40.103538Z",
     "iopub.status.idle": "2024-05-25T16:06:40.112651Z",
     "shell.execute_reply": "2024-05-25T16:06:40.112124Z",
     "shell.execute_reply.started": "2024-05-25T16:06:40.104252Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.31.5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('gradio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7df7d9-8605-45ae-b22f-b9524cd4d574",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "*Specific dependency for Phi3 small*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e50d8-e365-4917-ba7c-2efc7012eba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pytest==8.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e4454d0-0626-491d-9d0d-1a918f4ae234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:35:00.962500Z",
     "iopub.status.busy": "2024-05-25T14:35:00.961629Z",
     "iopub.status.idle": "2024-05-25T14:35:00.969877Z",
     "shell.execute_reply": "2024-05-25T14:35:00.969444Z",
     "shell.execute_reply.started": "2024-05-25T14:35:00.962464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.2.1'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('pytest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a4489-d48e-4111-995a-3d2029ab76d3",
   "metadata": {},
   "source": [
    "*Specific dependency for recent Mistral instruct models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0a2ec-c3a7-47ed-87f7-413de776673b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install mistral_common==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cddbef-a546-486c-afe8-e6b9feb79992",
   "metadata": {},
   "outputs": [],
   "source": [
    "version('mistral_common')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15ac64-6fd4-4cf0-b0ae-d186f57a5c62",
   "metadata": {},
   "source": [
    "*Specific dependency for Mixtral 8x7B HQQ quantization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d691b0-b1e5-46f2-b1a8-c53e441a93e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install hqq==0.1.7.post3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548cb3b2-2076-4aea-855c-ded7b3aca42b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:35:49.116775Z",
     "iopub.status.busy": "2024-06-02T06:35:49.116316Z",
     "iopub.status.idle": "2024-06-02T06:35:49.124387Z",
     "shell.execute_reply": "2024-06-02T06:35:49.123512Z",
     "shell.execute_reply.started": "2024-06-02T06:35:49.116742Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.7.post3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('hqq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978168a-4c2c-4d5b-9f0a-43d35739c1e1",
   "metadata": {},
   "source": [
    "WARNING: need to fix one bug in HQQ v0.1.7.post3\n",
    "\n",
    "> vi /workspace/instructlab-local/.venv/lib/python3.10/site-packages/hqq/core/utils.py\n",
    "\n",
    "Fix math ceil for torch.compile\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "...\n",
    "\n",
    "def is_divisible(val1: int, val2: int) -> bool:\n",
    "    return int(val2 * math.ceil(val1 / val2)) == val1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b7ba8-8e3b-45a7-925c-b8be74733b6f",
   "metadata": {},
   "source": [
    "## 2. Download student and teacher models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a1c6b2-2ba1-4672-bdc5-9f8e6b1ce281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T08:25:11.293644Z",
     "iopub.status.busy": "2024-06-02T08:25:11.293242Z",
     "iopub.status.idle": "2024-06-02T08:25:11.298446Z",
     "shell.execute_reply": "2024-06-02T08:25:11.297908Z",
     "shell.execute_reply.started": "2024-06-02T08:25:11.293628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"mistral\":\"mistralai/Mistral-7B-v0.3\",\n",
    "    \"mistral-instruct\":\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"llama3\":\"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"llama3-instruct\":\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"phi3-mini\":\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"phi3-small\":\"microsoft/Phi-3-small-8k-instruct\",\n",
    "    \"mixtral-q3\":\"mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ\",\n",
    "    \"mixtral-q2\":\"mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edca3ce-6bef-4059-b716-46bfb1ecac0f",
   "metadata": {},
   "source": [
    "**IMPORTANT: always set the local download cache directory explicitly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce758be1-bd73-4e9b-b957-ec5280fccb70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T08:25:11.594550Z",
     "iopub.status.busy": "2024-06-02T08:25:11.594148Z",
     "iopub.status.idle": "2024-06-02T08:25:11.597922Z",
     "shell.execute_reply": "2024-06-02T08:25:11.597416Z",
     "shell.execute_reply.started": "2024-06-02T08:25:11.594536Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download cache dir: /models/huggingface/transformers OK\n"
     ]
    }
   ],
   "source": [
    "DOWNLOAD_CACHE_DIR = \"/models/huggingface/transformers\"\n",
    "\n",
    "from pathlib import Path\n",
    "print(f\"Download cache dir: {DOWNLOAD_CACHE_DIR} {('OK' if Path(DOWNLOAD_CACHE_DIR).exists() else 'KO')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a68465-9def-473e-a44a-06e9ffcc2a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T08:25:11.724887Z",
     "iopub.status.busy": "2024-06-02T08:25:11.724406Z",
     "iopub.status.idle": "2024-06-02T08:25:13.530799Z",
     "shell.execute_reply": "2024-06-02T08:25:13.530151Z",
     "shell.execute_reply.started": "2024-06-02T08:25:11.724867Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/instructlab-local/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def get_model_path_and_size_on_disk(pretrained_model_id):    \n",
    "    model_config_file = cached_file(pretrained_model_id, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(os.path.dirname(model_config_file))    \n",
    "    total_size = get_directory_size(model_directory)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def get_gpu_free_memory():\n",
    "    return (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0))/memory_unit_mb\n",
    "\n",
    "def display_model_properties(model_name):\n",
    "    path,size = get_model_path_and_size_on_disk(model_name)\n",
    "    print(f\"- model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "    print(f\"- stored in directory: {path}\")\n",
    "\n",
    "    free_mem_mb = get_gpu_free_memory()\n",
    "    print(f\"- free memory after load: {free_mem_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d04107-2c4c-4203-8666-0f9e3e85abbe",
   "metadata": {},
   "source": [
    "Note: if you need to clean the huggingface cache before downloading new models ...\n",
    "\n",
    "```bash\n",
    "cd /workspace/instructlab-local/\n",
    "source .venv/bin/activate\n",
    "\n",
    "pip install huggingface_hub[cli]\n",
    "\n",
    "huggingface-cli delete-cache --dir /models/huggingface/transformers\n",
    "```\n",
    "\n",
    "Then navigate among model revisions with the arrow key, press space to select the revisions to delete, then Enter to delete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80b584-f182-4138-a1a6-fd9024ecc7fa",
   "metadata": {},
   "source": [
    "### 2.1 Mistral 7B base and instruct v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048d4c70-606c-4dfb-b23a-4f430982bbe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:44:41.958432Z",
     "iopub.status.busy": "2024-06-02T06:44:41.957791Z",
     "iopub.status.idle": "2024-06-02T06:44:53.185313Z",
     "shell.execute_reply": "2024-06-02T06:44:53.184837Z",
     "shell.execute_reply.started": "2024-06-02T06:44:41.958398Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbeb518051f4aafbe20159863e9680e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 13.50 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--mistralai--Mistral-7B-v0.3/snapshots\n",
      "- free memory after load: 10226.98 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = models[\"mistral\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347c3fb5-9137-4ad4-9901-fc802954adba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:45:03.867025Z",
     "iopub.status.busy": "2024-06-02T06:45:03.866138Z",
     "iopub.status.idle": "2024-06-02T06:45:44.160953Z",
     "shell.execute_reply": "2024-06-02T06:45:44.160134Z",
     "shell.execute_reply.started": "2024-06-02T06:45:03.866991Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6e149ec7044d83aa93bbcb188b1872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 27.00 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots\n",
      "- free memory after load: 10226.98 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = models[\"mistral-instruct\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64833757-09ff-4a9d-8a72-51be184b2411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:45:25.029192Z",
     "iopub.status.busy": "2024-05-25T13:45:25.028728Z",
     "iopub.status.idle": "2024-05-25T13:45:25.039994Z",
     "shell.execute_reply": "2024-05-25T13:45:25.038876Z",
     "shell.execute_reply.started": "2024-05-25T13:45:25.029158Z"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Llama 3 8B base and instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfec08e1-5aad-4eee-b3b0-ae5fcf7066a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:47:41.531328Z",
     "iopub.status.busy": "2024-06-02T06:47:41.530775Z",
     "iopub.status.idle": "2024-06-02T06:48:45.235888Z",
     "shell.execute_reply": "2024-06-02T06:48:45.235443Z",
     "shell.execute_reply.started": "2024-06-02T06:47:41.531293Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2537728d2c4a8ebf4d034c50f3551a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 14.97 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--meta-llama--Meta-Llama-3-8B/snapshots\n",
      "- free memory after load: 9246.98 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = models[\"llama3\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f591e2c3-0319-48e7-b976-141028563beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:49:09.940629Z",
     "iopub.status.busy": "2024-06-02T06:49:09.939781Z",
     "iopub.status.idle": "2024-06-02T06:51:46.266728Z",
     "shell.execute_reply": "2024-06-02T06:51:46.266187Z",
     "shell.execute_reply.started": "2024-06-02T06:49:09.940607Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f94bbe98e84a01a6e39616e58a465c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe41bdafd3548e7aac70aeecd981cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 29.93 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots\n",
      "- free memory after load: 9246.98 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = models[\"llama3-instruct\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9510097-b50c-4c96-84ea-cf355a0abd75",
   "metadata": {},
   "source": [
    "### 2.3 Phi 3 mini and small instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca53138-a480-4e7c-b52f-ac2a43193e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:56:35.226540Z",
     "iopub.status.busy": "2024-06-02T06:56:35.225903Z",
     "iopub.status.idle": "2024-06-02T06:57:10.833441Z",
     "shell.execute_reply": "2024-06-02T06:57:10.832939Z",
     "shell.execute_reply.started": "2024-06-02T06:56:35.226506Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857df2e6248f4c83b4d1a8f8636e0ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1fe645f8cf421eb88b51fda3364ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 14.24 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--microsoft--Phi-3-mini-4k-instruct/snapshots\n",
      "- free memory after load: 17275.10 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = models[\"phi3-mini\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8589c770-07fb-4f33-82de-ad454918927f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:57:32.865006Z",
     "iopub.status.busy": "2024-06-02T06:57:32.863674Z",
     "iopub.status.idle": "2024-06-02T06:58:31.632711Z",
     "shell.execute_reply": "2024-06-02T06:58:31.632239Z",
     "shell.execute_reply.started": "2024-06-02T06:57:32.864967Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- tokenization_phi3_small.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- configuration_phi3_small.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- triton_flash_blocksparse_attn.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- triton_blocksparse_attention_layer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- positional_embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- modeling_phi3_small.py\n",
      "- triton_flash_blocksparse_attn.py\n",
      "- triton_blocksparse_attention_layer.py\n",
      "- positional_embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3bbcd33cea478bbbba86c5f92fa9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41242375f7e241cd97397ec4b3fa4c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 27.54 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--microsoft--Phi-3-small-8k-instruct/snapshots\n",
      "- free memory after load: 10335.75 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = models[\"phi3-small\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\", trust_remote_code=True)\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad555a5-8ab8-43a5-9a13-8f9599c913f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4 Mixtral 8x7B instruct v0.1 HQQ - 3.5 bits and 2 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca72d5b-1b84-41c7-8342-c7ec2a2212d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T08:25:16.497360Z",
     "iopub.status.busy": "2024-06-02T08:25:16.496981Z",
     "iopub.status.idle": "2024-06-02T08:25:17.650697Z",
     "shell.execute_reply": "2024-06-02T08:25:17.649942Z",
     "shell.execute_reply.started": "2024-06-02T08:25:16.497342Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer as HQQAutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c205407e-84c1-47d4-a62f-ea62e93c7bbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T08:25:17.652668Z",
     "iopub.status.busy": "2024-06-02T08:25:17.652358Z",
     "iopub.status.idle": "2024-06-02T08:26:58.130918Z",
     "shell.execute_reply": "2024-06-02T08:26:58.129639Z",
     "shell.execute_reply.started": "2024-06-02T08:25:17.652651Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e802a289f6e46e28b352a7daa37a7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:03<00:00,  8.61it/s]\n",
      "100%|██████████| 32/32 [00:06<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- free memory after load: 4049.73 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = models[\"mixtral-q3\"]\n",
    "\n",
    "tokenizer = HQQAutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "model = HQQModelForCausalLM.from_quantized(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "\n",
    "# Download: 22.4 GB\n",
    "\n",
    "free_mem_mb = get_gpu_free_memory()\n",
    "print(f\"- free memory after load: {free_mem_mb:.2f} MB\")\n",
    "    \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c56f9447-b6db-44c6-abe2-07b6bdfd8bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T08:03:07.560166Z",
     "iopub.status.busy": "2024-06-02T08:03:07.559806Z",
     "iopub.status.idle": "2024-06-02T08:24:10.232723Z",
     "shell.execute_reply": "2024-06-02T08:24:10.232270Z",
     "shell.execute_reply.started": "2024-06-02T08:03:07.560149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5332dad9f2b4482486d8e878249d3164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59aa45457ea43f2a49c486dece1d1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84276f18fb24475af3e22079df04fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d84dfb408a492cad8b3b0d14a2b2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11abffee02fd4f0eac55d9a0d2adfaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea9249b6c13434c8f99110b91c8f7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/774 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af033437aeff4d22b30ca08c2405231b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13898aa274364858932bc08cef89408b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf6437a690f40f69af9c798cd47ef55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qmodel.pt:   0%|          | 0.00/18.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 74.10it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 493.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- free memory after load: 6608.79 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = models[\"mixtral-q2\"]\n",
    "\n",
    "tokenizer = HQQAutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "model = HQQModelForCausalLM.from_quantized(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "\n",
    "# Download: \n",
    "\n",
    "free_mem_mb = get_gpu_free_memory()\n",
    "print(f\"- free memory after load: {free_mem_mb:.2f} MB\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee0075-cc4c-4b20-b172-b1e800657f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructlab-local",
   "language": "python",
   "name": "instructlab-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
