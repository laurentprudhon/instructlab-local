{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4f2c52-5a4e-43d6-aa41-4d7ac8b34fb2",
   "metadata": {},
   "source": [
    "# Instructlab local - 01 Prepare environment\n",
    "\n",
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b38a0a8-cd35-4b9a-ae4c-f109464ce298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T08:29:40.559524Z",
     "iopub.status.busy": "2024-05-25T08:29:40.558647Z",
     "iopub.status.idle": "2024-05-25T08:29:40.693111Z",
     "shell.execute_reply": "2024-05-25T08:29:40.692454Z",
     "shell.execute_reply.started": "2024-05-25T08:29:40.559506Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, memory.total [MiB]\n",
      "NVIDIA GeForce RTX 4090, 24564 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total --format csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3eae68e-e9d7-43df-add9-beded295d8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T08:32:25.868135Z",
     "iopub.status.busy": "2024-05-25T08:32:25.867711Z",
     "iopub.status.idle": "2024-05-25T08:32:25.949395Z",
     "shell.execute_reply": "2024-05-25T08:32:25.948976Z",
     "shell.execute_reply.started": "2024-05-25T08:32:25.868116Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| NVIDIA-SMI 550.76.01              Driver Version: 552.44         CUDA Version: 12.4     |\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(\"nvidia-smi | grep 'CUDA Version'\", shell=True, capture_output=True, text=True)\n",
    "result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd6d9a-fa0e-4a21-999d-aab71b234a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torch==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de7967e-c770-4229-a972-7811c1d1262e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T17:49:37.190182Z",
     "iopub.status.busy": "2024-06-01T17:49:37.189927Z",
     "iopub.status.idle": "2024-06-01T17:49:37.195786Z",
     "shell.execute_reply": "2024-06-01T17:49:37.195363Z",
     "shell.execute_reply.started": "2024-06-01T17:49:37.190171Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "version('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e814904e-0279-4bdb-b754-1a09f17a4961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers==4.41.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa0afeb-54b1-417c-af01-237c83b245a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:27:00.937375Z",
     "iopub.status.busy": "2024-05-25T13:27:00.935618Z",
     "iopub.status.idle": "2024-05-25T13:27:00.945375Z",
     "shell.execute_reply": "2024-05-25T13:27:00.944933Z",
     "shell.execute_reply.started": "2024-05-25T13:27:00.937331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.41.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa295c95-58b2-4245-87d0-2bebb9f26d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install accelerate==0.30.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e98156-f674-4cba-a5e3-38be68f030cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:27:03.853477Z",
     "iopub.status.busy": "2024-05-25T13:27:03.852930Z",
     "iopub.status.idle": "2024-05-25T13:27:03.859330Z",
     "shell.execute_reply": "2024-05-25T13:27:03.858702Z",
     "shell.execute_reply.started": "2024-05-25T13:27:03.853455Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.30.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('accelerate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78fa79-57bb-4c5a-8f59-0046074a867c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install flash-attn==2.5.8 --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ff6406-e726-4b45-8171-7a8fa3fe8151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:28:34.432938Z",
     "iopub.status.busy": "2024-05-25T13:28:34.432001Z",
     "iopub.status.idle": "2024-05-25T13:28:34.442435Z",
     "shell.execute_reply": "2024-05-25T13:28:34.441380Z",
     "shell.execute_reply.started": "2024-05-25T13:28:34.432894Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.8'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('flash_attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1116b-2c3a-4153-8f0f-d99a638c9c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install vllm==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20798362-287e-4fbc-a0d0-ce08f0c3089c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T17:49:39.229065Z",
     "iopub.status.busy": "2024-06-01T17:49:39.228293Z",
     "iopub.status.idle": "2024-06-01T17:49:39.236765Z",
     "shell.execute_reply": "2024-06-01T17:49:39.235889Z",
     "shell.execute_reply.started": "2024-06-01T17:49:39.229034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cbd4e-b7d8-476e-a338-76f3413062ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install gradio==4.31.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cf7bb6a-ce7d-40ba-b4e1-400e65d41864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T16:06:40.104356Z",
     "iopub.status.busy": "2024-05-25T16:06:40.103538Z",
     "iopub.status.idle": "2024-05-25T16:06:40.112651Z",
     "shell.execute_reply": "2024-05-25T16:06:40.112124Z",
     "shell.execute_reply.started": "2024-05-25T16:06:40.104252Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.31.5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('gradio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7df7d9-8605-45ae-b22f-b9524cd4d574",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "*Specific dependency for Phi3 small*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e50d8-e365-4917-ba7c-2efc7012eba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pytest==8.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e4454d0-0626-491d-9d0d-1a918f4ae234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:35:00.962500Z",
     "iopub.status.busy": "2024-05-25T14:35:00.961629Z",
     "iopub.status.idle": "2024-05-25T14:35:00.969877Z",
     "shell.execute_reply": "2024-05-25T14:35:00.969444Z",
     "shell.execute_reply.started": "2024-05-25T14:35:00.962464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.2.1'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('pytest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15ac64-6fd4-4cf0-b0ae-d186f57a5c62",
   "metadata": {},
   "source": [
    "*Specific dependency for Mixtral 8x7B HQQ quantization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d691b0-b1e5-46f2-b1a8-c53e441a93e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install hqq==0.1.7.post2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "548cb3b2-2076-4aea-855c-ded7b3aca42b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:52:57.033329Z",
     "iopub.status.busy": "2024-05-25T14:52:57.031810Z",
     "iopub.status.idle": "2024-05-25T14:52:57.043207Z",
     "shell.execute_reply": "2024-05-25T14:52:57.042604Z",
     "shell.execute_reply.started": "2024-05-25T14:52:57.033282Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.7.post2'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('hqq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978168a-4c2c-4d5b-9f0a-43d35739c1e1",
   "metadata": {},
   "source": [
    "WARNING: need to patch a bug in HQQ v0.1.7.post2\n",
    "\n",
    "> vi /workspace/instructlab-local/.venv/lib/python3.10/site-packages/hqq/models/hf/base.py\n",
    "\n",
    "BaseHQQHFModel.create_model()\n",
    "\n",
    "```python\n",
    "    # Replace this line\n",
    "    def create_model(cls, save_dir, kwargs):\n",
    "        \n",
    "    # With this line\n",
    "    def create_model(cls, save_dir, **kwargs):    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b7ba8-8e3b-45a7-925c-b8be74733b6f",
   "metadata": {},
   "source": [
    "## 2. Download student and teacher models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a68465-9def-473e-a44a-06e9ffcc2a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T16:08:59.989711Z",
     "iopub.status.busy": "2024-05-25T16:08:59.988756Z",
     "iopub.status.idle": "2024-05-25T16:09:00.002201Z",
     "shell.execute_reply": "2024-05-25T16:09:00.001513Z",
     "shell.execute_reply.started": "2024-05-25T16:08:59.989684Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def get_model_path_and_size_on_disk(pretrained_model_id):    \n",
    "    model_config_file = cached_file(pretrained_model_id, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(os.path.dirname(model_config_file))    \n",
    "    total_size = get_directory_size(model_directory)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def get_gpu_free_memory():\n",
    "    return (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0))/memory_unit_mb\n",
    "\n",
    "def display_model_properties(model_name):\n",
    "    path,size = get_model_path_and_size_on_disk(model_name)\n",
    "    print(f\"- model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "    print(f\"- stored in directory: {path}\")\n",
    "\n",
    "    free_mem_mb = get_gpu_free_memory()\n",
    "    print(f\"- free memory after load: {free_mem_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d04107-2c4c-4203-8666-0f9e3e85abbe",
   "metadata": {},
   "source": [
    "Note: if you need to clean the huggingface cache before downloading new models ...\n",
    "\n",
    "```bash\n",
    "cd /workspace/instructlab-local/\n",
    "source .venv/bin/activate\n",
    "\n",
    "pip install huggingface_hub[cli]\n",
    "\n",
    "huggingface-cli delete-cache --dir /models/huggingface/transformers\n",
    "```\n",
    "\n",
    "Then navigate among model revisions with the arrow key, press space to select the revisions to delete, then Enter to delete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80b584-f182-4138-a1a6-fd9024ecc7fa",
   "metadata": {},
   "source": [
    "### 2.1 Mistral 7B base and instruct v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "048d4c70-606c-4dfb-b23a-4f430982bbe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:05:15.357928Z",
     "iopub.status.busy": "2024-05-25T14:05:15.357468Z",
     "iopub.status.idle": "2024-05-25T14:05:31.479986Z",
     "shell.execute_reply": "2024-05-25T14:05:31.479293Z",
     "shell.execute_reply.started": "2024-05-25T14:05:15.357898Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3994e8063fb94df0b14c95f67ddb26ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d3d145bd6b46fda2e75766fc70ff97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 13.50 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--mistralai--Mistral-7B-v0.3/snapshots\n",
      "- free memory after load: 10226.98 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "347c3fb5-9137-4ad4-9901-fc802954adba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:05:35.023221Z",
     "iopub.status.busy": "2024-05-25T14:05:35.022247Z",
     "iopub.status.idle": "2024-05-25T14:05:45.621928Z",
     "shell.execute_reply": "2024-05-25T14:05:45.621471Z",
     "shell.execute_reply.started": "2024-05-25T14:05:35.023181Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef40b11b20947558a94f8a9b6f0fae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1db93c41f0347dc93783c1f001ceb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 13.50 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots\n",
      "- free memory after load: 10226.98 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64833757-09ff-4a9d-8a72-51be184b2411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T13:45:25.029192Z",
     "iopub.status.busy": "2024-05-25T13:45:25.028728Z",
     "iopub.status.idle": "2024-05-25T13:45:25.039994Z",
     "shell.execute_reply": "2024-05-25T13:45:25.038876Z",
     "shell.execute_reply.started": "2024-05-25T13:45:25.029158Z"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Llama 3 8B base and instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfec08e1-5aad-4eee-b3b0-ae5fcf7066a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:22:45.559267Z",
     "iopub.status.busy": "2024-05-25T14:22:45.558485Z",
     "iopub.status.idle": "2024-05-25T14:23:02.831725Z",
     "shell.execute_reply": "2024-05-25T14:23:02.831160Z",
     "shell.execute_reply.started": "2024-05-25T14:22:45.559233Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4e94f4a6564b4c826f59c92abcb520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441fd42e76fa439dbe76d0dca5d3644c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 14.97 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--meta-llama--Meta-Llama-3-8B/snapshots\n",
      "- free memory after load: 9246.98 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f591e2c3-0319-48e7-b976-141028563beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:23:02.833636Z",
     "iopub.status.busy": "2024-05-25T14:23:02.833424Z",
     "iopub.status.idle": "2024-05-25T14:23:50.956511Z",
     "shell.execute_reply": "2024-05-25T14:23:50.956019Z",
     "shell.execute_reply.started": "2024-05-25T14:23:02.833626Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b6a93d59024a0d9c319056b4b5f87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2360bfb54fa3435ca7ac80b599f5716c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 14.97 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots\n",
      "- free memory after load: 9246.98 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9510097-b50c-4c96-84ea-cf355a0abd75",
   "metadata": {},
   "source": [
    "### 2.3 Phi 3 mini and small instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eca53138-a480-4e7c-b52f-ac2a43193e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:31:40.408150Z",
     "iopub.status.busy": "2024-05-25T14:31:40.407678Z",
     "iopub.status.idle": "2024-05-25T14:31:42.328683Z",
     "shell.execute_reply": "2024-05-25T14:31:42.328169Z",
     "shell.execute_reply.started": "2024-05-25T14:31:40.408120Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641db9949e0a44bd835b9b536f58cf8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111ce6747a294e2f86c2534b42933e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 7.12 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--microsoft--Phi-3-mini-4k-instruct/snapshots\n",
      "- free memory after load: 17275.10 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8589c770-07fb-4f33-82de-ad454918927f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T14:45:51.032805Z",
     "iopub.status.busy": "2024-05-25T14:45:51.031903Z",
     "iopub.status.idle": "2024-05-25T14:45:56.054117Z",
     "shell.execute_reply": "2024-05-25T14:45:56.053662Z",
     "shell.execute_reply.started": "2024-05-25T14:45:51.032758Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- tokenization_phi3_small.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- configuration_phi3_small.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-small-8k-instruct:\n",
      "- modeling_phi3_small.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6b8c5e683d4748a666bc1323010b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb8f02c6e5443e2bd9d0b6ccae8ceec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model files size   : 13.77 GB\n",
      "- stored in directory: /models/huggingface/transformers/models--microsoft--Phi-3-small-8k-instruct/snapshots\n",
      "- free memory after load: 10335.75 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/Phi-3-small-8k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\", trust_remote_code=True)\n",
    "\n",
    "display_model_properties(model_name)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad555a5-8ab8-43a5-9a13-8f9599c913f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4 Mixtral 8x7B instruct v0.1 HQQ - 3.5 bits and 2 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca72d5b-1b84-41c7-8342-c7ec2a2212d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T16:06:56.612936Z",
     "iopub.status.busy": "2024-05-25T16:06:56.611730Z",
     "iopub.status.idle": "2024-05-25T16:07:08.737100Z",
     "shell.execute_reply": "2024-05-25T16:07:08.736279Z",
     "shell.execute_reply.started": "2024-05-25T16:06:56.612894Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/instructlab-local/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer as HQQAutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c205407e-84c1-47d4-a62f-ea62e93c7bbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:31:31.406089Z",
     "iopub.status.busy": "2024-05-25T15:31:31.405794Z",
     "iopub.status.idle": "2024-05-25T15:33:29.567004Z",
     "shell.execute_reply": "2024-05-25T15:33:29.565301Z",
     "shell.execute_reply.started": "2024-05-25T15:31:31.406070Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a329ed85f714ba8b63a8cc533a868ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.34it/s]\n",
      "100%|██████████| 32/32 [00:08<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- free memory after load: 4049.73 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ\"\n",
    "\n",
    "tokenizer = HQQAutoTokenizer.from_pretrained(model_name)\n",
    "model = HQQModelForCausalLM.from_quantized(model_name)\n",
    "\n",
    "free_mem_mb = get_gpu_free_memory()\n",
    "print(f\"- free memory after load: {free_mem_mb:.2f} MB\")\n",
    "    \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f9447-b6db-44c6-abe2-07b6bdfd8bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ\"\n",
    "\n",
    "tokenizer = HQQAutoTokenizer.from_pretrained(model_name)\n",
    "model = HQQModelForCausalLM.from_quantized(model_name)\n",
    "\n",
    "free_mem_mb = get_gpu_free_memory()\n",
    "print(f\"- free memory after load: {free_mem_mb:.2f} MB\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f09cea4-d682-4bba-9b18-a87af48a1fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T16:09:31.199243Z",
     "iopub.status.busy": "2024-05-25T16:09:31.198410Z",
     "iopub.status.idle": "2024-05-25T16:09:31.206390Z",
     "shell.execute_reply": "2024-05-25T16:09:31.205673Z",
     "shell.execute_reply.started": "2024-05-25T16:09:31.199208Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- free memory after load: 6853.60 MB\n"
     ]
    }
   ],
   "source": [
    "free_mem_mb = get_gpu_free_memory()\n",
    "print(f\"- free memory after load: {free_mem_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b4422-f319-4f9b-b02e-208ed4a96d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructlab-local",
   "language": "python",
   "name": "instructlab-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
